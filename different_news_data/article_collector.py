import json
import re
import time
from dataclasses import dataclass
from typing import List, Dict, Optional
import feedparser
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import mysql.connector
import sys # sys ëª¨ë“ˆ ì¶”ê°€

# --- ì„¤ì • ---
MODEL_NAME = "intfloat/multilingual-e5-base"
TARGET_ARTICLES_PER_SIDE = 10
SIMILARITY_THRESHOLD = 0.60
DB_CONFIG = {
    'user': 'root',
    'password': '3302', # ë³¸ì¸ì˜ DB ë¹„ë°€ë²ˆí˜¸ë¡œ ë³€ê²½
    'host': '127.0.0.1',
    'database': 'different_news'
}

# =========================
# ğŸ“° RSS í”¼ë“œ ì •ë³´ ë° ë°ì´í„° êµ¬ì¡° (1ë‹¨ê³„ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼)
# =========================
Side = str
@dataclass
class Feed:
    source: str; source_domain: str; side: Side; url: str; section: str

FEEDS: List[Feed] = [
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/politic_news.xml", "ì •ì¹˜"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/economy_news.xml", "ê²½ì œ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/society_news.xml", "ì‚¬íšŒ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/culture_news.xml", "ë¬¸í™”"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/politics/", "ì •ì¹˜"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/economy/", "ê²½ì œ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/society/", "ì‚¬íšŒ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/culture/", "ë¬¸í™”"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/politics.xml", "ì •ì¹˜"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/economy.xml", "ê²½ì œ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/society.xml", "ì‚¬íšŒ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/culture.xml", "ë¬¸í™”"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/politics/?outputType=xml", "ì •ì¹˜"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/economy/?outputType=xml", "ê²½ì œ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/society/?outputType=xml", "ì‚¬íšŒ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/culture/?outputType=xml", "ë¬¸í™”"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì •ì¹˜&hl=ko&gl=KR&ceid=KR%3Ako", "ì •ì¹˜"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ê²½ì œ&hl=ko&gl=KR&ceid=KR%3Ako", "ê²½ì œ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì‚¬íšŒ&hl=ko&gl=KR&ceid=KR%3Ako", "ì‚¬íšŒ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ë¬¸í™”&hl=ko&gl=KR&ceid=KR%3Ako", "ë¬¸í™”"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/politics.xml", "ì •ì¹˜"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/economy.xml", "ê²½ì œ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/national.xml", "ì‚¬íšŒ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/culture.xml", "ë¬¸í™”"),
]

@dataclass
class Article:
    source: str; source_domain: str; side: Side; title: str; url: str
    published_at: Optional[str]; section: str; rss_desc: Optional[str] = None
    similarity: float = 0.0

def pull_feeds() -> List[Article]:
    all_articles: List[Article] = []
    # ... (1ë‹¨ê³„ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼) ...
    unique_titles = set()
    for f in tqdm(FEEDS, desc=" ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘"):
        feed = feedparser.parse(f.url)
        for entry in feed.entries:
            title = (getattr(entry, "title", "") or "").strip()
            if not title or title in unique_titles: continue
            unique_titles.add(title)
            link = (getattr(entry, "link", None) or "").strip()
            published = getattr(entry, "published", None) or getattr(entry, "updated", None)
            desc = re.sub(r"<[^>]+>", " ", getattr(entry, "summary", "")).strip()
            all_articles.append(Article(
                source=f.source, source_domain=f.source_domain, side=f.side,
                title=title, url=link, published_at=published, section=f.section, rss_desc=desc
            ))
        time.sleep(0.05)
    return all_articles

_MODEL = None
def get_model():
    global _MODEL
    if _MODEL is None: _MODEL = SentenceTransformer(MODEL_NAME)
    return _MODEL

def embed_texts(texts: List[str], is_query: bool = False) -> np.ndarray:
    model = get_model()
    prefix = "query: " if is_query else "passage: "
    prefixed_texts = [f"{prefix}{text[:512]}" for text in texts]
    vecs = model.encode(prefixed_texts, batch_size=128, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(vecs, dtype=np.float32)

# =========================
# ğŸ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =========================
def main():
    """DBì—ì„œ ë°œí–‰ëœ í† í”½ì˜ 'search_keywords'ë¥¼ ì´ìš©í•´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ë‹¤ì‹œ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        cnx = mysql.connector.connect(**DB_CONFIG)
        cursor = cnx.cursor(dictionary=True)
        print(" DB ì—°ê²° ì„±ê³µ.")

        cursor.execute("SELECT id, display_name, search_keywords FROM topics WHERE status = 'published'")
        published_topics = cursor.fetchall()
        if not published_topics:
            print(" ë°œí–‰ëœ í† í”½ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ì‚¬ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        print(f"â–¶ {len(published_topics)}ê°œì˜ ë°œí–‰ëœ í† í”½ì— ëŒ€í•œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
        articles = pull_feeds()
        if not articles: print(" ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."); return
        article_embeds = embed_texts([a.title for a in articles])

        for topic in published_topics:
            topic_id = topic['id']
            display_name = topic['display_name']
            print(f"\n--- í† í”½: '{display_name}' (ID #{topic_id}) ì²˜ë¦¬ ì¤‘ ---")
            
            # [ìˆ˜ì •] search_keywordsê°€ ë¹„ì–´ìˆëŠ”(None) ê²½ìš°ë¥¼ í™•ì¸í•˜ëŠ” ë¡œì§ ì¶”ê°€
            search_keywords_str = topic.get('search_keywords')
            if not search_keywords_str:
                print(f"  -  í† í”½ ID #{topic_id}ì— ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue # ë‹¤ìŒ í† í”½ìœ¼ë¡œ ë„˜ì–´ê°

            search_keywords = [kw.strip() for kw in search_keywords_str.split(',') if kw.strip()]
            if not search_keywords:
                print(f"  -  í† í”½ ID #{topic_id}ì— ìœ íš¨í•œ ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            keyword_embeds = embed_texts(search_keywords, is_query=True)
            topic_vector = np.mean(keyword_embeds, axis=0)

            # ... (ì´í•˜ ê¸°ì‚¬ ìˆ˜ì§‘ ë° DB ì €ì¥ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
            similarities = cosine_similarity(article_embeds, topic_vector.reshape(1, -1)).flatten()
            relevant_articles = []
            for i, article in enumerate(articles):
                if similarities[i] >= SIMILARITY_THRESHOLD:
                    article.similarity = float(similarities[i])
                    relevant_articles.append(article)
            
            left_articles = sorted([a for a in relevant_articles if a.side == 'LEFT'], key=lambda a: a.similarity, reverse=True)[:TARGET_ARTICLES_PER_SIDE]
            right_articles = sorted([a for a in relevant_articles if a.side == 'RIGHT'], key=lambda a: a.similarity, reverse=True)[:TARGET_ARTICLES_PER_SIDE]
            final_articles = left_articles + right_articles

            print(f"  - ì°¾ì€ ê¸°ì‚¬: ì¢Œ {len(left_articles)}ê°œ, ìš° {len(right_articles)}ê°œ. DBì— ì €ì¥í•©ë‹ˆë‹¤...")
            cursor.execute("DELETE FROM articles WHERE topic_id = %s", (topic_id,))
            insert_query = "INSERT INTO articles (topic_id, source, side, title, url, published_at, similarity) VALUES (%s, %s, %s, %s, %s, %s, %s)"
            for article in final_articles:
                cursor.execute(insert_query, (topic_id, article.source, article.side, article.title, article.url, article.published_at, article.similarity))
            cnx.commit()
            print(f"  ->  í† í”½ ID #{topic_id} ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ!")

    except mysql.connector.Error as err:
        print(f" ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {err}")
    finally:
        if 'cnx' in locals() and cnx.is_connected():
            cursor.close()
            cnx.close()
            print("\n DB ì—°ê²° ì¢…ë£Œ.")

if __name__ == '__main__':
    main()