import json
import re
import time
from collections import Counter
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import feedparser
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from konlpy.tag import Okt

# --- ì„¤ì • ---
MODEL_NAME = "intfloat/multilingual-e5-base"
MIN_CLUSTER_SIZE = 10
TOPIC_WORD_BLACKLIST = { "ì •ë¶€", "ëŒ€í†µë ¹ì‹¤", "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜", "êµ­íšŒ", "ì—¬ì•¼", "ì˜í˜¹",
                         "ë…¼ë€", "ê´€ê³„ì", "ë°œí‘œ", "ì‚¬ì§„", "ì†ë³´", "ë‹¨ë…", "ì˜¤ëŠ˜", "ë‚´ì¼", 
                         "ê´€ë ¨", "ì´ìŠˆ", "ì‚¬ì‹¤", "ìƒê°", "í•œêµ­", "ìš°ë¦¬", "ì¤‘ì•™", "ì¼ë³´", 
                         "ë‰´ìŠ¤", "ê¸°ì‚¬", "ì–¸ë¡ ", "ë³´ë„", "ê¸°ì", "ì·¨ì¬", "ì „ë¬¸ê°€", "ì „ë¬¸ê°€ë“¤",
                         "í•˜ì´ë¼ì´íŠ¸", "ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”"}
# --- RSS í”¼ë“œ ì •ë³´ ---
Side = str
@dataclass
class Feed:
    source: str; source_domain: str; side: Side; url: str; section: str
FEEDS: List[Feed] = [
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/politic_news.xml", "ì •ì¹˜"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/economy_news.xml", "ê²½ì œ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/society_news.xml", "ì‚¬íšŒ"),
    Feed("ê²½í–¥ì‹ ë¬¸", "khan.co.kr", "LEFT", "https://www.khan.co.kr/rss/rssdata/culture_news.xml", "ë¬¸í™”"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/politics/", "ì •ì¹˜"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/economy/", "ê²½ì œ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/society/", "ì‚¬íšŒ"),
    Feed("í•œê²¨ë ˆ", "hani.co.kr", "LEFT", "https://www.hani.co.kr/rss/culture/", "ë¬¸í™”"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/politics.xml", "ì •ì¹˜"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/economy.xml", "ê²½ì œ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/society.xml", "ì‚¬íšŒ"),
    Feed("ì˜¤ë§ˆì´ë‰´ìŠ¤", "ohmynews.com", "LEFT", "http://rss.ohmynews.com/rss/culture.xml", "ë¬¸í™”"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/politics/?outputType=xml", "ì •ì¹˜"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/economy/?outputType=xml", "ê²½ì œ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/society/?outputType=xml", "ì‚¬íšŒ"),
    Feed("ì¡°ì„ ì¼ë³´", "chosun.com", "RIGHT", "https://www.chosun.com/arc/outboundfeeds/rss/category/culture/?outputType=xml", "ë¬¸í™”"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì •ì¹˜&hl=ko&gl=KR&ceid=KR%3Ako", "ì •ì¹˜"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ê²½ì œ&hl=ko&gl=KR&ceid=KR%3Ako", "ê²½ì œ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ì‚¬íšŒ&hl=ko&gl=KR&ceid=KR%3Ako", "ì‚¬íšŒ"),
    Feed("ì¤‘ì•™ì¼ë³´", "joongang.co.kr", "RIGHT", "https://news.google.com/rss/search?q=site:joongang.co.kr%20ë¬¸í™”&hl=ko&gl=KR&ceid=KR%3Ako", "ë¬¸í™”"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/politics.xml", "ì •ì¹˜"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/economy.xml", "ê²½ì œ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/national.xml", "ì‚¬íšŒ"),
    Feed("ë™ì•„ì¼ë³´", "donga.com", "RIGHT", "https://rss.donga.com/culture.xml", "ë¬¸í™”"),
]
@dataclass
class Article:
    source: str; source_domain: str; side: Side; title: str; url: str
    published_at: Optional[str]; section: str; rss_desc: Optional[str] = None
# ... (pull_feeds, get_model, embed_texts, get_okt, extract_topic_keyword_and_desc í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ìŒ)
def pull_feeds() -> List[Article]:
    all_articles: List[Article] = []
    unique_titles = set()
    for f in tqdm(FEEDS, desc="ğŸ“° RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘"):
        feed = feedparser.parse(f.url)
        for entry in feed.entries:
            title = (getattr(entry, "title", "") or "").strip()
            if not title or title in unique_titles: continue
            unique_titles.add(title)
            link = (getattr(entry, "link", None) or "").strip()
            published = getattr(entry, "published", None) or getattr(entry, "updated", None)
            desc = re.sub(r"<[^>]+>", " ", getattr(entry, "summary", "")).strip()
            all_articles.append(Article(source=f.source, source_domain=f.source_domain, side=f.side, title=title, url=link, published_at=published, section=f.section, rss_desc=desc))
        time.sleep(0.05)
    return all_articles
_MODEL = None
def get_model():
    global _MODEL
    if _MODEL is None: _MODEL = SentenceTransformer(MODEL_NAME)
    return _MODEL
def embed_texts(texts: List[str]) -> np.ndarray:
    model = get_model()
    prefixed_texts = [f"passage: {text[:512]}" for text in texts]
    vecs = model.encode(prefixed_texts, batch_size=128, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(vecs, dtype=np.float32)
_OKT = None
def get_okt():
    global _OKT
    if _OKT is None: _OKT = Okt()
    return _OKT
def extract_topic_keyword_and_desc(titles: List[str]) -> Optional[Tuple[str, str]]:
    okt = get_okt()
    full_text = " ".join(titles)
    nouns = okt.nouns(full_text)
    filtered_nouns = [n for n in nouns if len(n) > 1 and n.lower() not in TOPIC_WORD_BLACKLIST]
    if not filtered_nouns: return None
    counts = Counter(filtered_nouns)
    most_common = counts.most_common(4)
    if not most_common: return None
    core_keyword = most_common[0][0]
    sub_desc_keywords = [kw for kw, count in most_common[1:4]]
    sub_description = ", ".join(sub_desc_keywords)
    return core_keyword, sub_description
# ...

def main():
    # 1. RSS ìˆ˜ì§‘ ë° ê°€ì¤‘ì¹˜ ì ìš© ì„ë² ë”©
    articles = pull_feeds()
    if len(articles) < 50:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    titles = [a.title for a in articles]
    descs  = [a.rss_desc or "" for a in articles]
    title_embeds = embed_texts(titles)
    desc_embeds  = embed_texts(descs)
    article_embeds = 0.7 * title_embeds + 0.3 * desc_embeds
    article_embeds = article_embeds / (np.linalg.norm(article_embeds, axis=1, keepdims=True) + 1e-8)

    # 2. êµ°ì§‘ ìƒì„±
    n_clusters = 25
    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)
    labels = kmeans.fit_predict(article_embeds)
    clusters: Dict[int, List[int]] = {i: [] for i in range(n_clusters)}
    for i, label in enumerate(labels):
        clusters[label].append(i)

    # 3. [ìˆ˜ì •] í›„ë³´ ìƒì„± ë° ì ìˆ˜ ê³„ì‚°ì„ í•˜ë‚˜ì˜ ë°˜ë³µë¬¸ìœ¼ë¡œ í†µí•©
    topic_candidates = []
    centroids_map = {i: np.mean(article_embeds[idxs], axis=0) for i, idxs in clusters.items() if len(idxs) > 0}

    for i, idxs in tqdm(clusters.items(), desc="ğŸ” í† í”½ í›„ë³´ ìƒì„± ë° ì ìˆ˜ ê³„ì‚° ì¤‘"):
        if len(idxs) < MIN_CLUSTER_SIZE:
            continue

        group_articles = [articles[i] for i in idxs]
        keyword_result = extract_topic_keyword_and_desc([a.title for a in group_articles])
        if not keyword_result:
            continue

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        centroid = centroids_map[i]
        cohesion = np.mean(cosine_similarity(article_embeds[idxs], centroid.reshape(1, -1)))
        
        side_counts = Counter(a.side for a in group_articles)
        left = side_counts.get("LEFT", 0)
        right = side_counts.get("RIGHT", 0)
        balance = min(left, right) / max(left, right) if left > 0 and right > 0 else 0.0
        
        coverage = min(1.0, (len(group_articles) / 50.0) * 0.7 + (len({a.source_domain for a in group_articles}) / 6.0) * 0.3)
        
        score = (0.40 * cohesion) + (0.30 * balance) + (0.30 * coverage)

        topic_candidates.append({
            "core_keyword": keyword_result[0],
            "sub_description": keyword_result[1],
            "_score": score,
            "stats": {
                "total_articles": len(group_articles),
                "left_articles": left,
                "right_articles": right,
                "source_count": len({a.source_domain for a in group_articles}),
            },
            "sample_titles": [a.title for a in group_articles[:5]],
        })

    # 4. ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ 10ê°œ ì„ íƒ
    topic_candidates.sort(key=lambda t: t["_score"], reverse=True)
    top_ten_candidates = topic_candidates[:10]

    # 5. ìµœì¢… ê²°ê³¼ë¬¼ ì •ë¦¬
    final_results = []
    for cand in top_ten_candidates:
        # DBì— ë„£ì„ ê¹”ë”í•œ ë°ì´í„°ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
        final_results.append({
            "core_keyword": cand["core_keyword"],
            "sub_description": cand["sub_description"],
            "stats": cand["stats"],
            "sample_titles": cand["sample_titles"],
        })

    print(f"\nâœ… ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ {len(final_results)}ê°œ í† í”½ì„ ì„ ë°œí–ˆìŠµë‹ˆë‹¤.")
    with open("suggested_topics.json", "w", encoding="utf-8") as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! 'suggested_topics.json' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()